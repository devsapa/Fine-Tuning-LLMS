{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"padding: 10px; border: 1px solid black;\">\n",
    "<img src=\"images/MLU-NEW-logo.png\" alt=\"drawing\" width=\"400\"/> <br/>\n",
    "\n",
    "\n",
    "# <a name=\"0\">MLU LLM Workshop </a>\n",
    "## <a name=\"0\">Lab 1: Inference with Pretrained Models </a>\n",
    "\n",
    "This notebook exemplifies how to use pretrained large languages models (LLMs) for processing either a specific text or a tailored dataset. By utilizing LLMs, we can perform a variety of natural language processing (NLP) tasks such as text classification, sentiment analysis, and text generation. LLMs have been pre-trained on enormous amounts of data, making them highly effective in understanding the nuances of language and generating coherent responses.\n",
    "\n",
    "\n",
    "1. <a href=\"#1\">Import libraries</a>\n",
    "2. <a href=\"#2\">Load an LLM</a>\n",
    "3. <a href=\"#3\">Batch inference with text generation</a>\n",
    "4. <a href=\"#4\">LLM inference with a customized dataset</a>\n",
    "5. <a href=\"#5\">Quizzes</a>\n",
    "\n",
    "__Jupyter notebooks environment__:\n",
    "\n",
    "* Jupiter notebooks allow creating and sharing documents that contain both code and rich text cells. If you are not familiar with Jupiter notebooks, read more [here](https://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/what_is_jupyter.html). \n",
    "* This is a quick-start demo to bring you up to speed on coding and experimenting with machine learning. Move through the notebook __from top to bottom__. \n",
    "* Run each code cell to see its output. To run a cell, click within the cell and press __Shift+Enter__, or click __Run__ from the top of the page menu. \n",
    "* A `[*]` symbol next to the cell indicates the code is still running. A `[#]` symbol, where # is an integer, indicates it is finished.\n",
    "* Beware, __some code cells might take longer to run__, sometimes 5-10 minutes (depending on the task, installing packages and libraries, training models, etc.)\n",
    "    \n",
    "    \n",
    "Please work top to bottom of this notebook and don't skip sections as this could lead to error messages due to missing code.\n",
    "\n",
    "---\n",
    "\n",
    "You will be presented with two kinds of exercises throughout the notebook: activities and challenges. <br/>\n",
    "\n",
    "| <img style=\"float: center;\" src=\"./images/activity.png\" alt=\"Activity\" width=\"125\"/>| <img style=\"float: center;\" src=\"./images/challenge.png\" alt=\"Challenge\" width=\"125\"/>|\n",
    "| --- | --- |\n",
    "|<p style=\"text-align:center;\">No coding is needed for an activity. You try to understand a concept, <br/>answer questions, or run a code cell.</p> |<p style=\"text-align:center;\">Challenges are where you test your understanding by taking a short quiz.</p> |\n",
    "\n",
    "----    \n",
    "\n",
    "\n",
    "Let's start by loading some libraries and packages!\n",
    "\n",
    "---\n",
    "\n",
    "### <a name=\"1\">Import libraries</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "\n",
    "First, let's install and import the necessary libraries, including the Hugging Face Transformers library and the PyTorch library, which is a dependency for Transformers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip3 install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/ec2-user/anaconda3/envs/pytorch_p310/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### <a name=\"#2\">Load an LLM</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "---\n",
    "\n",
    "The LLMs we are going to use are models from the [Dolly](https://github.com/databrickslabs/dolly) family, a set of instruction-following LLMs commercially open-sourced by Databricks. There are three sizes of models: 3 billions (`3b`),  7 billions (`7b`) and  12 billions (`12b`). The Dolly family models are derived from EleutherAI’s Pythia-12b and fine-tuned on a [~15K record instruction corpus](https://huggingface.co/datasets/databricks/databricks-dolly-15k) generated by Databricks employees and released under a permissive license (CC-BY-SA).\n",
    "\n",
    "First, let's pick a model to load. The below code initializes a tokenizer and a base model using the `Dolly-v2-3b` model from the Hugging Face Transformers library. The tokenizer converts raw text into tokens, and the base model generates text based on a given prompt. By following the instructions outlined above, you can correctly instantiate these components and leverage their functionality in your code.\n",
    "\n",
    "The following code initializes an inference pipeline using the `pipeline()` function from the Transformers library. The pipeline is created with the purpose of text generation with the following parameters:\n",
    "- `model=\"databricks/dolly-v2-3b\"` specifies the name of the pre-trained model to be used.\n",
    "- `device_map=\"auto\"` specifies the device where the model will be loaded. Setting it to \"auto\" allows the library to automatically select the appropriate device (e.g., CPU or GPU) based on availability.\n",
    "- `torch_dtype=torch.float16` specifies the data type for the model's weights and activations. In this case, the model will use half-precision floating-point numbers (`torch.float16`) to save memory and potentially speed up computation. Note that not all models support this data type.\n",
    "- `trust_remote_code=True` determines whether to trust remote code from the model's repository. By setting it to True, the pipeline will allow remote code execution, such as custom methods or functions provided by the model's repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instruct_pipeline = pipeline(model=\"databricks/dolly-v2-3b\", \n",
    "                             device_map=\"auto\",\n",
    "                             torch_dtype=torch.float16, \n",
    "                             trust_remote_code=True,\n",
    "                             )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "### <a name=\"#3\">LLM inference with text generation</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "---\n",
    "\n",
    "Let's test our `instruct_pipeline` with its text generation functionalities!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
    "    <h2><i>Try it Yourself!</i></h2>\n",
    "    <br>\n",
    "    <p style=\"text-align:center;margin:auto;\"><img src=\"./images/activity.png\" alt=\"Activity\" width=\"100\" /> </p>\n",
    "    <p style=\" text-align: center; margin: auto;\">Try different prompts and observe the responses generated by the model.</p>\n",
    "    <p style=\" text-align: center; margin: auto;\"><b>Note: Results may not be factually accurate and may be based on false assumptions.</b></p>\n",
    "    <br>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Here is a list of solutions that come pre-built with Amazon SageMaker JumpStart:\\n1. MLFlow\\n2. Gluon\\n3. Dali\\n4. ML Flow Model Builisher\\n5. MLFlow Model Builisher'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"What solutions come pre-built with Amazon SageMaker JumpStart?\"\n",
    "output1 = instruct_pipeline(prompt)\n",
    "output1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above outputs are hard to read. Let's display it in a easy to read Markdown format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here is a list of solutions that come pre-built with Amazon SageMaker JumpStart:\n",
       "1. MLFlow\n",
       "2. Gluon\n",
       "3. Dali\n",
       "4. ML Flow Model Builisher\n",
       "5. MLFlow Model Builisher"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown(output1[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "\n",
    "### <a name=\"#4\">Batch inference with a customized dataset</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is Amazon SageMaker?</td>\n",
       "      <td>Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In which Regions is Amazon SageMaker available?\\r\\n</td>\n",
       "      <td>For a list of the supported Amazon SageMaker AWS Regions, please visit the AWS Regional Services page. Also, for more information, see Regional endpoints in the AWS general reference guide.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the service availability of Amazon SageMaker?\\r\\n</td>\n",
       "      <td>Amazon SageMaker is designed for high availability. There are no maintenance windows or scheduled downtimes. SageMaker APIs run in Amazon’s proven, high-availability data centers, with service stack replication configured across three facilities in each AWS Region to provide fault tolerance in the event of a server failure or Availability Zone outage.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does Amazon SageMaker secure my code?</td>\n",
       "      <td>Amazon SageMaker stores code in ML storage volumes, secured by security groups and optionally encrypted at rest.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What security measures does Amazon SageMaker have?</td>\n",
       "      <td>Amazon SageMaker ensures that ML model artifacts and other system artifacts are encrypted in transit and at rest. Requests to the SageMaker API and console are made over a secure (SSL) connection. You pass AWS Identity and Access Management roles to SageMaker to provide permissions to access resources on your behalf for training and deployment. You can use encrypted Amazon Simple Storage Service (Amazon S3) buckets for model artifacts and data, as well as pass an AWS Key Management Service (KMS) key to SageMaker notebooks, training jobs, and endpoints, to encrypt the attached ML storage volume. Amazon SageMaker also supports Amazon Virtual Private Cloud (VPC) and AWS PrivateLink support.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 instruction  \\\n",
       "0                                  What is Amazon SageMaker?   \n",
       "1        In which Regions is Amazon SageMaker available?\\r\\n   \n",
       "2  What is the service availability of Amazon SageMaker?\\r\\n   \n",
       "3                  How does Amazon SageMaker secure my code?   \n",
       "4         What security measures does Amazon SageMaker have?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   response  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             For a list of the supported Amazon SageMaker AWS Regions, please visit the AWS Regional Services page. Also, for more information, see Regional endpoints in the AWS general reference guide.  \n",
       "2                                                                                                                                                                                                                                                                                                                                                         Amazon SageMaker is designed for high availability. There are no maintenance windows or scheduled downtimes. SageMaker APIs run in Amazon’s proven, high-availability data centers, with service stack replication configured across three facilities in each AWS Region to provide fault tolerance in the event of a server failure or Availability Zone outage.  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Amazon SageMaker stores code in ML storage volumes, secured by security groups and optionally encrypted at rest.  \n",
       "4  Amazon SageMaker ensures that ML model artifacts and other system artifacts are encrypted in transit and at rest. Requests to the SageMaker API and console are made over a secure (SSL) connection. You pass AWS Identity and Access Management roles to SageMaker to provide permissions to access resources on your behalf for training and deployment. You can use encrypted Amazon Simple Storage Service (Amazon S3) buckets for model artifacts and data, as well as pass an AWS Key Management Service (KMS) key to SageMaker notebooks, training jobs, and endpoints, to encrypt the attached ML storage volume. Amazon SageMaker also supports Amazon Virtual Private Cloud (VPC) and AWS PrivateLink support.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test = pd.read_csv(\"data/amazon_sagemaker_faqs.csv\")\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's batch inference on the question dataset. Running this cell might take upto ~15 minutes depending on the number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.9 s, sys: 0 ns, total: 30.9 s\n",
      "Wall time: 30.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Set number of samples for inference\n",
    "num_samples = 10\n",
    "\n",
    "test = test.sample(num_samples)\n",
    "predictions = instruct_pipeline(list(test['instruction'].head(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the predictions to the original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>response</th>\n",
       "      <th>LLM prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>What is Amazon SageMaker Studio Lab?</td>\n",
       "      <td>Amazon SageMaker Studio Lab is a free ML development environment that provides the compute, storage (up to 15 GB), and security—all at no cost—for anyone to learn and experiment with ML. All you need to get started is a valid email ID; you don’t need to configure infrastructure or manage identity and access or even sign up for an AWS account. SageMaker Studio Lab accelerates model building through GitHub integration, and it comes preconfigured with the most popular ML tools, frameworks, and libraries to get you started immediately. SageMaker Studio Lab automatically saves your work so you don’t need to restart between sessions. It’s as easy as closing your laptop and coming back later.</td>\n",
       "      <td>Amazon SageMaker Studio Lab makes it easy to develop and deploy machine learning (ML) applications on Amazon SageMaker. With Amazon SageMaker Studio Lab, you can easily create an Amazon SageMaker model with pre-trained ML models, train and validate your models, and deploy your models to Amazon SageMaker through your AWS account.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>What type of endpoints does SageMaker Inference Recommender support?</td>\n",
       "      <td>Currently we support only real-time endpoints.</td>\n",
       "      <td>SageMaker Inference Recommender supports the endpoint for retrieving a set of machine-learned recommendations from SageMaker Sentiment Inference Service.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>How do I maintain consistency between online and offline features?</td>\n",
       "      <td>Amazon SageMaker Feature Store automatically maintains consistency between online and offline features without additional management or code. SageMaker Feature Store is fully managed and maintains consistency across training and inference environments.</td>\n",
       "      <td>One of the key challenges when building a distributed system is maintaining consistency between online and offline features. One common approach to handling this challenge is to handle offline data in a read-only state, and provide the read-write APIs to allow for modifications.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>What is Amazon SageMaker Inference Recommender?</td>\n",
       "      <td>Amazon SageMaker Inference Recommender is a new capability of Amazon SageMaker that reduces the time required to get ML models in production by automating performance benchmarking and tuning model performance across SageMaker ML instances. You can now use SageMaker Inference Recommender to deploy your model to an endpoint that delivers the best performance and minimizes cost. You can get started with SageMaker Inference Recommender in minutes while selecting an instance type and get recommendations for optimal endpoint configurations within hours, eliminating weeks of manual testing and tuning time. With SageMaker Inference Recommender, you pay only for the SageMaker ML instances used during load testing, and there are no additional charges.</td>\n",
       "      <td>The Inference Recommender enables users to perform data science with their favorite Hadoop frameworks, without having to rely on complex or specialized systems. It uses the Deep Learning Libraries for computer vision and machine learning tasks, which are optimized for speed and memory consumption and provide an efficient and scalable solution for large-scale recommender systems.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Can I optimize multiple objectives simultaneously, such as optimizing a model to be both fast and accurate?\\r\\n</td>\n",
       "      <td>Not at this time. Currently, you need to specify a single objective metric to optimize or change your algorithm code to emit a new metric, which is a weighted average between two or more useful metrics, and have the tuning process optimize towards that objective metric.</td>\n",
       "      <td>No, you can optimize one objective at a time, and each objective needs to be defined well. If you are optimizing a model to be fast, and the objective is based on loss, and the loss is not going to be accurate, the model will be slow even if it has the correct output. On the other hand, if the loss is based on the reference values, the model might give the wrong output for correct values, but it might be fast. So, if both the loss and accuracy are important for you, it is better to optimize one at a time and do a proper data tuning.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                         instruction  \\\n",
       "70                                                                              What is Amazon SageMaker Studio Lab?   \n",
       "122                                             What type of endpoints does SageMaker Inference Recommender support?   \n",
       "50                                                How do I maintain consistency between online and offline features?   \n",
       "118                                                                  What is Amazon SageMaker Inference Recommender?   \n",
       "100  Can I optimize multiple objectives simultaneously, such as optimizing a model to be both fast and accurate?\\r\\n   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            response  \\\n",
       "70                                                            Amazon SageMaker Studio Lab is a free ML development environment that provides the compute, storage (up to 15 GB), and security—all at no cost—for anyone to learn and experiment with ML. All you need to get started is a valid email ID; you don’t need to configure infrastructure or manage identity and access or even sign up for an AWS account. SageMaker Studio Lab accelerates model building through GitHub integration, and it comes preconfigured with the most popular ML tools, frameworks, and libraries to get you started immediately. SageMaker Studio Lab automatically saves your work so you don’t need to restart between sessions. It’s as easy as closing your laptop and coming back later.   \n",
       "122                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Currently we support only real-time endpoints.   \n",
       "50                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Amazon SageMaker Feature Store automatically maintains consistency between online and offline features without additional management or code. SageMaker Feature Store is fully managed and maintains consistency across training and inference environments.   \n",
       "118  Amazon SageMaker Inference Recommender is a new capability of Amazon SageMaker that reduces the time required to get ML models in production by automating performance benchmarking and tuning model performance across SageMaker ML instances. You can now use SageMaker Inference Recommender to deploy your model to an endpoint that delivers the best performance and minimizes cost. You can get started with SageMaker Inference Recommender in minutes while selecting an instance type and get recommendations for optimal endpoint configurations within hours, eliminating weeks of manual testing and tuning time. With SageMaker Inference Recommender, you pay only for the SageMaker ML instances used during load testing, and there are no additional charges.   \n",
       "100                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Not at this time. Currently, you need to specify a single objective metric to optimize or change your algorithm code to emit a new metric, which is a weighted average between two or more useful metrics, and have the tuning process optimize towards that objective metric.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 LLM prediction  \n",
       "70                                                                                                                                                                                                                   Amazon SageMaker Studio Lab makes it easy to develop and deploy machine learning (ML) applications on Amazon SageMaker. With Amazon SageMaker Studio Lab, you can easily create an Amazon SageMaker model with pre-trained ML models, train and validate your models, and deploy your models to Amazon SageMaker through your AWS account.  \n",
       "122                                                                                                                                                                                                                                                                                                                                                                                                   SageMaker Inference Recommender supports the endpoint for retrieving a set of machine-learned recommendations from SageMaker Sentiment Inference Service.  \n",
       "50                                                                                                                                                                                                                                                                      One of the key challenges when building a distributed system is maintaining consistency between online and offline features. One common approach to handling this challenge is to handle offline data in a read-only state, and provide the read-write APIs to allow for modifications.  \n",
       "118                                                                                                                                                               The Inference Recommender enables users to perform data science with their favorite Hadoop frameworks, without having to rely on complex or specialized systems. It uses the Deep Learning Libraries for computer vision and machine learning tasks, which are optimized for speed and memory consumption and provide an efficient and scalable solution for large-scale recommender systems.  \n",
       "100  No, you can optimize one objective at a time, and each objective needs to be defined well. If you are optimizing a model to be fast, and the objective is based on loss, and the loss is not going to be accurate, the model will be slow even if it has the correct output. On the other hand, if the loss is based on the reference values, the model might give the wrong output for correct values, but it might be fast. So, if both the loss and accuracy are important for you, it is better to optimize one at a time and do a proper data tuning.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions_clean = [x[0]['generated_text'] for x in predictions]\n",
    "test['LLM prediction'] = predictions_clean\n",
    "\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"5\">Quizzes</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Well done on completing the lab! Now, it's time for a brief knowledge assessment.\n",
    "\n",
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
    "    <h2><i>Try it Yourself!</i></h2>\n",
    "    <br>\n",
    "    <p style=\"text-align:center;margin:auto;\"><img src=\"./images/challenge.png\" alt=\"Challenge\" width=\"100\" /> </p>\n",
    "    <p style=\" text-align: center; margin: auto;\">Answer the following questions to test your understanding of using pre-trained LLMs for inference.</p>\n",
    "    <br>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>var Quiz=function(){\"use strict\";var M=document.createElement(\"style\");M.textContent=`.quiz-wrapper.svelte-fk6ar3{padding:1rem}.footer.svelte-fk6ar3{display:flex;align-items:center}h2.svelte-fk6ar3{font-size:1.5rem;margin-bottom:2rem;color:#232f3e}p.svelte-fk6ar3{font-size:16px}.options.svelte-fk6ar3{display:grid;grid-template-columns:repeat(2,50%);grid-template-rows:repeat(2,1fr);width:100%;margin:auto;justify-content:center}.mlu-quizquestion-option-button.svelte-fk6ar3{padding:1rem;margin:.5rem}.submit-button.svelte-fk6ar3{padding:1rem;margin:.5rem;width:90px;color:#fff;background-color:coral}.active.svelte-fk6ar3{background-color:#232f3e;color:#fff}.correct-answer.svelte-fk6ar3{background-color:green}.incorrect-answer.svelte-fk6ar3{background-color:red;text-decoration:line-through}.available.svelte-fk6ar3{pointer-events:none;opacity:.6}\n",
       "`,document.head.appendChild(M);function I(){}function P(e){return e()}function W(){return Object.create(null)}function O(e){e.forEach(P)}function X(e){return typeof e==\"function\"}function Z(e,t){return e!=e?t==t:e!==t||e&&typeof e==\"object\"||typeof e==\"function\"}function x(e){return Object.keys(e).length===0}function m(e,t){e.appendChild(t)}function j(e,t,n){e.insertBefore(t,n||null)}function z(e){e.parentNode&&e.parentNode.removeChild(e)}function ee(e,t){for(let n=0;n<e.length;n+=1)e[n]&&e[n].d(t)}function k(e){return document.createElement(e)}function A(e){return document.createTextNode(e)}function S(){return A(\" \")}function te(){return A(\"\")}function Y(e,t,n,r){return e.addEventListener(t,n,r),()=>e.removeEventListener(t,n,r)}function v(e,t,n){n==null?e.removeAttribute(t):e.getAttribute(t)!==n&&e.setAttribute(t,n)}function ne(e){return Array.from(e.childNodes)}function T(e,t){t=\"\"+t,e.wholeText!==t&&(e.data=t)}function p(e,t,n){e.classList[n?\"add\":\"remove\"](t)}let L;function N(e){L=e}const $=[],D=[],Q=[],F=[],re=Promise.resolve();let B=!1;function oe(){B||(B=!0,re.then(H))}function R(e){Q.push(e)}const G=new Set;let E=0;function H(){if(E!==0)return;const e=L;do{try{for(;E<$.length;){const t=$[E];E++,N(t),le(t.$$)}}catch(t){throw $.length=0,E=0,t}for(N(null),$.length=0,E=0;D.length;)D.pop()();for(let t=0;t<Q.length;t+=1){const n=Q[t];G.has(n)||(G.add(n),n())}Q.length=0}while($.length);for(;F.length;)F.pop()();B=!1,G.clear(),N(e)}function le(e){if(e.fragment!==null){e.update(),O(e.before_update);const t=e.dirty;e.dirty=[-1],e.fragment&&e.fragment.p(e.ctx,t),e.after_update.forEach(R)}}const ie=new Set;function se(e,t){e&&e.i&&(ie.delete(e),e.i(t))}function ce(e,t,n,r){const{fragment:l,after_update:i}=e.$$;l&&l.m(t,n),r||R(()=>{const s=e.$$.on_mount.map(P).filter(X);e.$$.on_destroy?e.$$.on_destroy.push(...s):O(s),e.$$.on_mount=[]}),i.forEach(R)}function fe(e,t){const n=e.$$;n.fragment!==null&&(O(n.on_destroy),n.fragment&&n.fragment.d(t),n.on_destroy=n.fragment=null,n.ctx=[])}function ue(e,t){e.$$.dirty[0]===-1&&($.push(e),oe(),e.$$.dirty.fill(0)),e.$$.dirty[t/31|0]|=1<<t%31}function ae(e,t,n,r,l,i,s,g=[-1]){const c=L;N(e);const o=e.$$={fragment:null,ctx:[],props:i,update:I,not_equal:l,bound:W(),on_mount:[],on_destroy:[],on_disconnect:[],before_update:[],after_update:[],context:new Map(t.context||(c?c.$$.context:[])),callbacks:W(),dirty:g,skip_bound:!1,root:t.target||c.$$.root};s&&s(o.root);let b=!1;if(o.ctx=n?n(e,t.props||{},(a,q,...y)=>{const h=y.length?y[0]:q;return o.ctx&&l(o.ctx[a],o.ctx[a]=h)&&(!o.skip_bound&&o.bound[a]&&o.bound[a](h),b&&ue(e,a)),q}):[],o.update(),b=!0,O(o.before_update),o.fragment=r?r(o.ctx):!1,t.target){if(t.hydrate){const a=ne(t.target);o.fragment&&o.fragment.l(a),a.forEach(z)}else o.fragment&&o.fragment.c();t.intro&&se(e.$$.fragment),ce(e,t.target,t.anchor,t.customElement),H()}N(c)}class de{$destroy(){fe(this,1),this.$destroy=I}$on(t,n){if(!X(n))return I;const r=this.$$.callbacks[t]||(this.$$.callbacks[t]=[]);return r.push(n),()=>{const l=r.indexOf(n);l!==-1&&r.splice(l,1)}}$set(t){this.$$set&&!x(t)&&(this.$$.skip_bound=!0,this.$$set(t),this.$$.skip_bound=!1)}}const be=\"\";function J(e,t,n){const r=e.slice();return r[11]=t[n],r[13]=n,r}function K(e){let t,n=e[11]+\"\",r,l,i,s;function g(){return e[9](e[13])}return{c(){t=k(\"button\"),r=A(n),l=S(),v(t,\"class\",\"mlu-quizquestion-option-button button svelte-fk6ar3\"),p(t,\"active\",e[5]===e[13]),p(t,\"correct-answer\",e[1]&&e[5]===e[13]&&e[2]==e[0].correctIndex),p(t,\"incorrect-answer\",e[1]&&e[5]===e[13]&&e[2]!=e[0].correctIndex),p(t,\"available\",e[4]&&e[1])},m(c,o){j(c,t,o),m(t,r),m(t,l),i||(s=Y(t,\"click\",g),i=!0)},p(c,o){e=c,o&1&&n!==(n=e[11]+\"\")&&T(r,n),o&32&&p(t,\"active\",e[5]===e[13]),o&39&&p(t,\"correct-answer\",e[1]&&e[5]===e[13]&&e[2]==e[0].correctIndex),o&39&&p(t,\"incorrect-answer\",e[1]&&e[5]===e[13]&&e[2]!=e[0].correctIndex),o&18&&p(t,\"available\",e[4]&&e[1])},d(c){c&&z(t),i=!1,s()}}}function U(e){let t;function n(i,s){return i[3]==!0?he:_e}let r=n(e),l=r(e);return{c(){l.c(),t=te()},m(i,s){l.m(i,s),j(i,t,s)},p(i,s){r!==(r=n(i))&&(l.d(1),l=r(i),l&&(l.c(),l.m(t.parentNode,t)))},d(i){l.d(i),i&&z(t)}}}function _e(e){let t;return{c(){t=k(\"p\"),t.textContent=\"This is not the correct answer. Try again!\",v(t,\"class\",\"svelte-fk6ar3\")},m(n,r){j(n,t,r)},d(n){n&&z(t)}}}function he(e){let t;return{c(){t=k(\"p\"),t.textContent=\"Good! You got the correct answer.\",v(t,\"class\",\"svelte-fk6ar3\")},m(n,r){j(n,t,r)},d(n){n&&z(t)}}}function me(e){let t,n,r=e[0].question+\"\",l,i,s,g,c,o,b=e[1]?\"Retry\":\"Submit\",a,q,y,h,C=e[0].options,d=[];for(let f=0;f<C.length;f+=1)d[f]=K(J(e,C,f));let _=e[1]&&U(e);return{c(){t=k(\"div\"),n=k(\"h2\"),l=A(r),i=S(),s=k(\"div\");for(let f=0;f<d.length;f+=1)d[f].c();g=S(),c=k(\"div\"),o=k(\"button\"),a=A(b),q=S(),_&&_.c(),v(n,\"class\",\"svelte-fk6ar3\"),v(s,\"class\",\"options svelte-fk6ar3\"),v(o,\"class\",\"submit-button svelte-fk6ar3\"),p(o,\"available\",!e[4]),v(c,\"class\",\"footer svelte-fk6ar3\"),v(t,\"class\",\"quiz-wrapper svelte-fk6ar3\")},m(f,w){j(f,t,w),m(t,n),m(n,l),m(t,i),m(t,s);for(let u=0;u<d.length;u+=1)d[u].m(s,null);m(t,g),m(t,c),m(c,o),m(o,a),m(c,q),_&&_.m(c,null),y||(h=Y(o,\"click\",e[10]),y=!0)},p(f,[w]){if(w&1&&r!==(r=f[0].question+\"\")&&T(l,r),w&311){C=f[0].options;let u;for(u=0;u<C.length;u+=1){const V=J(f,C,u);d[u]?d[u].p(V,w):(d[u]=K(V),d[u].c(),d[u].m(s,null))}for(;u<d.length;u+=1)d[u].d(1);d.length=C.length}w&2&&b!==(b=f[1]?\"Retry\":\"Submit\")&&T(a,b),w&16&&p(o,\"available\",!f[4]),f[1]?_?_.p(f,w):(_=U(f),_.c(),_.m(c,null)):_&&(_.d(1),_=null)},i:I,o:I,d(f){f&&z(t),ee(d,f),_&&_.d(),y=!1,h()}}}function pe(e,t,n){let{question:r={question:\"Who didn't attend this meeting?\",options:[\"Xin\",\"Anand\",\"Brent\"],correctIndex:2}}=t,l=!1,i=-1,s=\"no\",g=!1,c;function o(){n(4,g=!1),n(1,l=!1),n(2,i=-1),n(5,c=-1),n(3,s=\"no\")}function b(){n(1,l=!0),n(3,s=i==r.correctIndex)}function a(h){n(4,g=!0),n(2,i=h),n(5,c=h)}const q=h=>a(h),y=()=>l?o():b();return e.$$set=h=>{\"question\"in h&&n(0,r=h.question)},[r,l,i,s,g,c,o,b,a,q,y]}class ge extends de{constructor(t){super(),ae(this,t,pe,me,Z,{question:0})}}return ge}();\n",
       "</script>\n",
       "        \n",
       "        <div id=\"Quiz-393a9045\"></div>\n",
       "        <script>\n",
       "        (() => {\n",
       "            var data = {\n",
       "\"question\": {\n",
       "\"question\": \"Which family did the pre-trained model used in this notebook belong to?\",\n",
       "\"options\": [\n",
       "\"LLaMa\",\n",
       "\"Dolly\",\n",
       "\"Bert\",\n",
       "\"GPT\"\n",
       "],\n",
       "\"correctIndex\": 1\n",
       "}\n",
       "};\n",
       "            window.Quiz_data = data;\n",
       "            var Quiz_inst = new Quiz({\n",
       "                \"target\": document.getElementById(\"Quiz-393a9045\"),\n",
       "                \"props\": data\n",
       "            });\n",
       "        })();\n",
       "        </script>\n",
       "        \n",
       "        "
      ],
      "text/plain": [
       "<mlu_utils.quiz_questions.Quiz at 0x7fe8ac98ad40>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlu_utils.quiz_questions import *\n",
    "lab1_question1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>var Quiz=function(){\"use strict\";var M=document.createElement(\"style\");M.textContent=`.quiz-wrapper.svelte-fk6ar3{padding:1rem}.footer.svelte-fk6ar3{display:flex;align-items:center}h2.svelte-fk6ar3{font-size:1.5rem;margin-bottom:2rem;color:#232f3e}p.svelte-fk6ar3{font-size:16px}.options.svelte-fk6ar3{display:grid;grid-template-columns:repeat(2,50%);grid-template-rows:repeat(2,1fr);width:100%;margin:auto;justify-content:center}.mlu-quizquestion-option-button.svelte-fk6ar3{padding:1rem;margin:.5rem}.submit-button.svelte-fk6ar3{padding:1rem;margin:.5rem;width:90px;color:#fff;background-color:coral}.active.svelte-fk6ar3{background-color:#232f3e;color:#fff}.correct-answer.svelte-fk6ar3{background-color:green}.incorrect-answer.svelte-fk6ar3{background-color:red;text-decoration:line-through}.available.svelte-fk6ar3{pointer-events:none;opacity:.6}\n",
       "`,document.head.appendChild(M);function I(){}function P(e){return e()}function W(){return Object.create(null)}function O(e){e.forEach(P)}function X(e){return typeof e==\"function\"}function Z(e,t){return e!=e?t==t:e!==t||e&&typeof e==\"object\"||typeof e==\"function\"}function x(e){return Object.keys(e).length===0}function m(e,t){e.appendChild(t)}function j(e,t,n){e.insertBefore(t,n||null)}function z(e){e.parentNode&&e.parentNode.removeChild(e)}function ee(e,t){for(let n=0;n<e.length;n+=1)e[n]&&e[n].d(t)}function k(e){return document.createElement(e)}function A(e){return document.createTextNode(e)}function S(){return A(\" \")}function te(){return A(\"\")}function Y(e,t,n,r){return e.addEventListener(t,n,r),()=>e.removeEventListener(t,n,r)}function v(e,t,n){n==null?e.removeAttribute(t):e.getAttribute(t)!==n&&e.setAttribute(t,n)}function ne(e){return Array.from(e.childNodes)}function T(e,t){t=\"\"+t,e.wholeText!==t&&(e.data=t)}function p(e,t,n){e.classList[n?\"add\":\"remove\"](t)}let L;function N(e){L=e}const $=[],D=[],Q=[],F=[],re=Promise.resolve();let B=!1;function oe(){B||(B=!0,re.then(H))}function R(e){Q.push(e)}const G=new Set;let E=0;function H(){if(E!==0)return;const e=L;do{try{for(;E<$.length;){const t=$[E];E++,N(t),le(t.$$)}}catch(t){throw $.length=0,E=0,t}for(N(null),$.length=0,E=0;D.length;)D.pop()();for(let t=0;t<Q.length;t+=1){const n=Q[t];G.has(n)||(G.add(n),n())}Q.length=0}while($.length);for(;F.length;)F.pop()();B=!1,G.clear(),N(e)}function le(e){if(e.fragment!==null){e.update(),O(e.before_update);const t=e.dirty;e.dirty=[-1],e.fragment&&e.fragment.p(e.ctx,t),e.after_update.forEach(R)}}const ie=new Set;function se(e,t){e&&e.i&&(ie.delete(e),e.i(t))}function ce(e,t,n,r){const{fragment:l,after_update:i}=e.$$;l&&l.m(t,n),r||R(()=>{const s=e.$$.on_mount.map(P).filter(X);e.$$.on_destroy?e.$$.on_destroy.push(...s):O(s),e.$$.on_mount=[]}),i.forEach(R)}function fe(e,t){const n=e.$$;n.fragment!==null&&(O(n.on_destroy),n.fragment&&n.fragment.d(t),n.on_destroy=n.fragment=null,n.ctx=[])}function ue(e,t){e.$$.dirty[0]===-1&&($.push(e),oe(),e.$$.dirty.fill(0)),e.$$.dirty[t/31|0]|=1<<t%31}function ae(e,t,n,r,l,i,s,g=[-1]){const c=L;N(e);const o=e.$$={fragment:null,ctx:[],props:i,update:I,not_equal:l,bound:W(),on_mount:[],on_destroy:[],on_disconnect:[],before_update:[],after_update:[],context:new Map(t.context||(c?c.$$.context:[])),callbacks:W(),dirty:g,skip_bound:!1,root:t.target||c.$$.root};s&&s(o.root);let b=!1;if(o.ctx=n?n(e,t.props||{},(a,q,...y)=>{const h=y.length?y[0]:q;return o.ctx&&l(o.ctx[a],o.ctx[a]=h)&&(!o.skip_bound&&o.bound[a]&&o.bound[a](h),b&&ue(e,a)),q}):[],o.update(),b=!0,O(o.before_update),o.fragment=r?r(o.ctx):!1,t.target){if(t.hydrate){const a=ne(t.target);o.fragment&&o.fragment.l(a),a.forEach(z)}else o.fragment&&o.fragment.c();t.intro&&se(e.$$.fragment),ce(e,t.target,t.anchor,t.customElement),H()}N(c)}class de{$destroy(){fe(this,1),this.$destroy=I}$on(t,n){if(!X(n))return I;const r=this.$$.callbacks[t]||(this.$$.callbacks[t]=[]);return r.push(n),()=>{const l=r.indexOf(n);l!==-1&&r.splice(l,1)}}$set(t){this.$$set&&!x(t)&&(this.$$.skip_bound=!0,this.$$set(t),this.$$.skip_bound=!1)}}const be=\"\";function J(e,t,n){const r=e.slice();return r[11]=t[n],r[13]=n,r}function K(e){let t,n=e[11]+\"\",r,l,i,s;function g(){return e[9](e[13])}return{c(){t=k(\"button\"),r=A(n),l=S(),v(t,\"class\",\"mlu-quizquestion-option-button button svelte-fk6ar3\"),p(t,\"active\",e[5]===e[13]),p(t,\"correct-answer\",e[1]&&e[5]===e[13]&&e[2]==e[0].correctIndex),p(t,\"incorrect-answer\",e[1]&&e[5]===e[13]&&e[2]!=e[0].correctIndex),p(t,\"available\",e[4]&&e[1])},m(c,o){j(c,t,o),m(t,r),m(t,l),i||(s=Y(t,\"click\",g),i=!0)},p(c,o){e=c,o&1&&n!==(n=e[11]+\"\")&&T(r,n),o&32&&p(t,\"active\",e[5]===e[13]),o&39&&p(t,\"correct-answer\",e[1]&&e[5]===e[13]&&e[2]==e[0].correctIndex),o&39&&p(t,\"incorrect-answer\",e[1]&&e[5]===e[13]&&e[2]!=e[0].correctIndex),o&18&&p(t,\"available\",e[4]&&e[1])},d(c){c&&z(t),i=!1,s()}}}function U(e){let t;function n(i,s){return i[3]==!0?he:_e}let r=n(e),l=r(e);return{c(){l.c(),t=te()},m(i,s){l.m(i,s),j(i,t,s)},p(i,s){r!==(r=n(i))&&(l.d(1),l=r(i),l&&(l.c(),l.m(t.parentNode,t)))},d(i){l.d(i),i&&z(t)}}}function _e(e){let t;return{c(){t=k(\"p\"),t.textContent=\"This is not the correct answer. Try again!\",v(t,\"class\",\"svelte-fk6ar3\")},m(n,r){j(n,t,r)},d(n){n&&z(t)}}}function he(e){let t;return{c(){t=k(\"p\"),t.textContent=\"Good! You got the correct answer.\",v(t,\"class\",\"svelte-fk6ar3\")},m(n,r){j(n,t,r)},d(n){n&&z(t)}}}function me(e){let t,n,r=e[0].question+\"\",l,i,s,g,c,o,b=e[1]?\"Retry\":\"Submit\",a,q,y,h,C=e[0].options,d=[];for(let f=0;f<C.length;f+=1)d[f]=K(J(e,C,f));let _=e[1]&&U(e);return{c(){t=k(\"div\"),n=k(\"h2\"),l=A(r),i=S(),s=k(\"div\");for(let f=0;f<d.length;f+=1)d[f].c();g=S(),c=k(\"div\"),o=k(\"button\"),a=A(b),q=S(),_&&_.c(),v(n,\"class\",\"svelte-fk6ar3\"),v(s,\"class\",\"options svelte-fk6ar3\"),v(o,\"class\",\"submit-button svelte-fk6ar3\"),p(o,\"available\",!e[4]),v(c,\"class\",\"footer svelte-fk6ar3\"),v(t,\"class\",\"quiz-wrapper svelte-fk6ar3\")},m(f,w){j(f,t,w),m(t,n),m(n,l),m(t,i),m(t,s);for(let u=0;u<d.length;u+=1)d[u].m(s,null);m(t,g),m(t,c),m(c,o),m(o,a),m(c,q),_&&_.m(c,null),y||(h=Y(o,\"click\",e[10]),y=!0)},p(f,[w]){if(w&1&&r!==(r=f[0].question+\"\")&&T(l,r),w&311){C=f[0].options;let u;for(u=0;u<C.length;u+=1){const V=J(f,C,u);d[u]?d[u].p(V,w):(d[u]=K(V),d[u].c(),d[u].m(s,null))}for(;u<d.length;u+=1)d[u].d(1);d.length=C.length}w&2&&b!==(b=f[1]?\"Retry\":\"Submit\")&&T(a,b),w&16&&p(o,\"available\",!f[4]),f[1]?_?_.p(f,w):(_=U(f),_.c(),_.m(c,null)):_&&(_.d(1),_=null)},i:I,o:I,d(f){f&&z(t),ee(d,f),_&&_.d(),y=!1,h()}}}function pe(e,t,n){let{question:r={question:\"Who didn't attend this meeting?\",options:[\"Xin\",\"Anand\",\"Brent\"],correctIndex:2}}=t,l=!1,i=-1,s=\"no\",g=!1,c;function o(){n(4,g=!1),n(1,l=!1),n(2,i=-1),n(5,c=-1),n(3,s=\"no\")}function b(){n(1,l=!0),n(3,s=i==r.correctIndex)}function a(h){n(4,g=!0),n(2,i=h),n(5,c=h)}const q=h=>a(h),y=()=>l?o():b();return e.$$set=h=>{\"question\"in h&&n(0,r=h.question)},[r,l,i,s,g,c,o,b,a,q,y]}class ge extends de{constructor(t){super(),ae(this,t,pe,me,Z,{question:0})}}return ge}();\n",
       "</script>\n",
       "        \n",
       "        <div id=\"Quiz-33901c57\"></div>\n",
       "        <script>\n",
       "        (() => {\n",
       "            var data = {\n",
       "\"question\": {\n",
       "\"question\": \"Which library provided the 'pipeline' module used in this notebook?\",\n",
       "\"options\": [\n",
       "\"langchain\",\n",
       "\"autogluon\",\n",
       "\"sagemaker\",\n",
       "\"transformers\"\n",
       "],\n",
       "\"correctIndex\": 3\n",
       "}\n",
       "};\n",
       "            window.Quiz_data = data;\n",
       "            var Quiz_inst = new Quiz({\n",
       "                \"target\": document.getElementById(\"Quiz-33901c57\"),\n",
       "                \"props\": data\n",
       "            });\n",
       "        })();\n",
       "        </script>\n",
       "        \n",
       "        "
      ],
      "text/plain": [
       "<mlu_utils.quiz_questions.Quiz at 0x7fe8ac98b9a0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab1_question2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's restart the kernel to release CPU and GPU memory for the next lab.\n",
    "\n",
    "**You might see a pop up indicating the kernel has been restarted.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import IPython\n",
    "# IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"padding: 10px; border: 1px solid black;\">\n",
    "<img src=\"images/MLU-NEW-logo.png\" alt=\"drawing\" width=\"400\"/> <br/>\n",
    "\n",
    "# Thank you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
