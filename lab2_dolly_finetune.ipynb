{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fad5b3f-8b6d-4c84-95e0-ae7a2d8fddcc",
   "metadata": {},
   "source": [
    "<p style=\"padding: 10px; border: 1px solid black;\">\n",
    "<img src=\"images/MLU-NEW-logo.png\" alt=\"drawing\" width=\"400\"/> <br/>\n",
    "\n",
    "\n",
    "# <a name=\"0\">MLU LLM Workshop </a>\n",
    "## <a name=\"0\">Lab 2: Finetune a Pretrained Model </a>\n",
    "    \n",
    "In this notebook, we will explore finetuning a pretrained large languages model (LLM), a powerful technique in the field of generative AI. LLMs have been pre-trained on enormous amounts of data, making them highly effective in understanding the nuances of language and generating coherent responses. These models have learned to extract useful features and patterns from the data, making them a valuable resource for various machine learning tasks.\n",
    "\n",
    "Finetuning, also known as transfer learning, allows us to leverage the knowledge gained by a pretrained model and apply it to a different but related task. Instead of training a model from scratch, we start with a pretrained model and modify it to adapt to our specific problem domain. This approach not only saves significant computational resources but also benefits from the generalization capabilities of the pretrained model.\n",
    "\n",
    "In this notebook, we will walk through the process of finetuning a pretrained model step by step. We will cover the following key aspects:\n",
    "\n",
    "1. <a href=\"#1\">Import libraries</a>\n",
    "2. <a href=\"#2\">Prepare the training dataset</a>\n",
    "3. <a href=\"#3\">Load a pretrained LLM</a>\n",
    "4. <a href=\"#4\">Define the trainer and finetuned the LLM</a>\n",
    "5. <a href=\"#5\">Inference with the finetuned model</a>\n",
    "6. <a href=\"#6\">Quizzes</a>\n",
    "\n",
    "\n",
    "Please work top to bottom of this notebook and don't skip sections as this could lead to error messages due to missing code.\n",
    "\n",
    "---\n",
    "\n",
    "You will be presented with two kinds of exercises throughout the notebook: activities and challenges. <br/>\n",
    "\n",
    "| <img style=\"float: center;\" src=\"./images/activity.png\" alt=\"Activity\" width=\"125\"/>| <img style=\"float: center;\" src=\"./images/challenge.png\" alt=\"Challenge\" width=\"125\"/>|\n",
    "| --- | --- |\n",
    "|<p style=\"text-align:center;\">No coding is needed for an activity. You try to understand a concept, <br/>answer questions, or run a code cell.</p> |<p style=\"text-align:center;\">Challenges are where you test your understanding by taking a short quiz.</p> |\n",
    "\n",
    "----        \n",
    "\n",
    "Before running this notebook, let's quickly run the following command to check the GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10e7342f-cd0a-476e-82d6-be96030ba945",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug 23 16:32:49 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   34C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f76a73-7aca-4ffa-a324-083c9f46c174",
   "metadata": {},
   "source": [
    "**If you see your CUDA memory is occupied by more than half (like the image below), then you need to shutdown other running notebooks.**\n",
    "\n",
    "<p style=\"padding: 10px; border: 1px solid black;\">\n",
    "<img src=\"images/cuda_memory.png\" alt=\"drawing\" width=\"800\"/> <br/>\n",
    "\n",
    "    \n",
    "---\n",
    "\n",
    "### <a name=\"1\">Import libraries</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "First, let's import the necessary libraries, including the Hugging Face Transformers library and the PyTorch library, which is a dependency for Transformers. If you haven't install the packages, uncomment the below line and install them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dec27fc9-3f94-40aa-bbbc-46c59c60416a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "\n",
    "!pip3 install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc6cc815-2012-427f-ae5d-4551671c9683",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Any, Dict, List, Tuple, Union\n",
    "from datasets import Dataset, load_dataset, disable_caching\n",
    "disable_caching() ## disable huggingface cache\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TextDataset\n",
    "from transformers import pipeline\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import accelerate\n",
    "import bitsandbytes\n",
    "\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb64de6-aa77-469a-bd9d-66c47f3fac8b",
   "metadata": {},
   "source": [
    "### <a name=\"2\">Prepare the training dataset</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Second, let's load and view the dataset. We will use [Amazon SageMaker FAQs](https://aws.amazon.com/sagemaker/faqs/) as our main dataset. The dataset has two columns `instruction` and `response`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bb59e63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7005350a4841490abfe5ce98e352dbfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fc863ee2748457db3e1317aed05187a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f57b6117c94062ab71883efca16697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'response'],\n",
       "    num_rows: 154\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker_faqs_dataset = load_dataset(\"csv\", \n",
    "                                      data_files='data/amazon_sagemaker_faqs.csv')['train']\n",
    "sagemaker_faqs_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf48fa29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'What is Amazon SageMaker?',\n",
       " 'response': 'Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker_faqs_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b657f0-30e0-46b5-a25e-68ac0810a4c4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "To finetune our LLM, we need to decorate our instruction dataset with a prompt like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cac3fc5-2f3d-4950-ab00-0e77faa49419",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Below is an instruction that describes a task. Write a response that appropriately completes the request. Instruction: {instruction}\n",
       " Response:{response}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request. Instruction: {instruction}\\n Response:\"\"\"\n",
    "answer_template = \"\"\"{response}\"\"\"\n",
    "\n",
    "Markdown(prompt_template + answer_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3899dc75-0e78-4781-989a-e29281fcb01c",
   "metadata": {},
   "source": [
    "Let's feed the templates to our dataset via the below function named `_add_text`. It takes a record as input. The function first checks if both the instruction and response fields are not empty. If either of them is empty, it raises a ValueError with a corresponding error message. If both fields have values, the function creates a new \"text\" field in the record by formatting them using given `prompt_template` and `answer_template`. We also add the instruction and the response as additional fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "111fc930-2588-4a45-aefa-3eaa30cfb10e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _add_text(rec):\n",
    "    instruction = rec[\"instruction\"]\n",
    "    response = rec[\"response\"]\n",
    "    \n",
    "    if not instruction:\n",
    "        raise ValueError(f\"Expected an instruction in: {rec}\")\n",
    "\n",
    "    if not response:\n",
    "        raise ValueError(f\"Expected a response in: {rec}\")\n",
    "\n",
    "    rec[\"prompt\"] = prompt_template.format(instruction=instruction)\n",
    "    rec[\"answer\"] = answer_template.format(response=response)\n",
    "    rec[\"text\"] = rec[\"prompt\"] + rec[\"answer\"]\n",
    "\n",
    "    return rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b2e2677-5762-444a-bb4c-be2690aaa33b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "061475d77f534e528db43ee2f2d6e124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'instruction': 'What is Amazon SageMaker?',\n",
       " 'response': 'Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.',\n",
       " 'prompt': 'Below is an instruction that describes a task. Write a response that appropriately completes the request. Instruction: What is Amazon SageMaker?\\n Response:',\n",
       " 'answer': 'Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.',\n",
       " 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request. Instruction: What is Amazon SageMaker?\\n Response:Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker_faqs_dataset = sagemaker_faqs_dataset.map(_add_text)\n",
    "sagemaker_faqs_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0eb01d-7eaa-4ca3-8db7-d401de768c39",
   "metadata": {},
   "source": [
    "Use `Markdown` to neatly display the text with PROMPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb332c19-0645-4a23-a191-dc597ba4a708",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Below is an instruction that describes a task. Write a response that appropriately completes the request. Instruction: What is Amazon SageMaker?\n",
       " Response:Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(sagemaker_faqs_dataset[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9777454-403a-41b1-9c45-3bfbaf0d855d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### <a name=\"#3\">Load a pretrained LLM</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "---\n",
    "Similar to Lab1, we will continue working on the `Dolly-v2-3b` (the 3 billions parameter pretrainedmodel from [Dolly](https://github.com/databrickslabs/dolly) family). The Dolly family models are derived from EleutherAIâ€™s Pythia-12b and fine-tuned on a [~15K record instruction corpus](https://huggingface.co/datasets/databricks/databricks-dolly-15k) generated by Databricks employees and released under a permissive license (CC-BY-SA).\n",
    "\n",
    "First, let's initialize a tokenizer and a base model using the `Dolly-v2-3b` model from the Hugging Face Transformers library. The tokenizer converts raw text into tokens, and the base model generates text based on a given prompt. By following the instructions outlined above, you can correctly instantiate these components and leverage their functionality in your code.\n",
    "\n",
    "\n",
    "The `AutoTokenizer.from_pretrained()` function is used to instantiate the tokenizer. \n",
    "- `padding_side=\"left\"` specifies the side of the sequences where padding tokens will be added. In this case, padding tokens will be added to the left side of each sequence. \n",
    "- The `eos_token` is a special token representing the end of a sequence. By assigning it to the `pad_token`, any padding tokens added during tokenization will also be considered as end-of-sequence tokens. This can be useful when generating text using the model, as it will know when to stop generating text after encountering padding tokens.\n",
    "\n",
    "After execution, the `tokenizer` object will be initialized and ready to use for tokenizing text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba182a1a-3819-4452-9dfa-cad2cceb40d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbaf5a14f28d4284b7660152232af5e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/450 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d1706a5274641afaf46707951cc7dc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18c9006c893c4c21a5d074007119baa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/228 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"databricks/dolly-v2-3b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384c94f1",
   "metadata": {},
   "source": [
    "Now we initialize and download a base model using the `AutoModelForCausalLM` class provided by the Transformers library. Base models are responsible for generating text based on a given prompt.\n",
    "\n",
    "The `AutoModelForCausalLM.from_pretrained()` function is used to instantiate the base model. \n",
    "- `use_cache=False` determines whether the model should use the local cache when loading pre-trained weights. By setting it to False, the cache will not be used, and the model will always download the weights from the remote source.\n",
    "- `device_map=\"auto\"` specifies the device where the model will be loaded. Setting it to \"auto\" allows the library to automatically select the appropriate device (e.g., CPU or GPU) based on availability.\n",
    "- `load_in_8bit` indicates to loading the model weights in 8-bit format, which is a technique to further reduce memory usage and improve performance.\n",
    "- The `resize_token_embeddings()` method resizes the model's token embeddings to match the vocabulary size, allowing the model to correctly interpret and generate text based on the tokens used by the tokenizer.\n",
    "\n",
    "\n",
    "After execution, the `base_model` object will be initialized and ready to use for generating text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b1ea8a2-3696-4281-ab46-27021b324cb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abe5fbdaa6af4a83b835a8302be6d397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/819 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5f6ae51dad94679a06e5d0a0b7d77c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/5.68G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    # use_cache=False,\n",
    "    device_map=\"auto\", #\"balanced\",\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfa87de-548d-4cdb-a1fb-a00202714add",
   "metadata": {},
   "source": [
    "#### Prepare model for training\n",
    "Some pre-processing needs to be done before training such an int8 model using peft, therefore let's import an utiliy function `prepare_model_for_int8_training` that will:\n",
    "\n",
    "- Casts all the non `int8` modules to full precision (fp32) for stability\n",
    "- Add a forward_hook to the input embedding layer to enable gradient computation of the input hidden states\n",
    "- Enable gradient checkpointing for more memory-efficient training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d86de4c7-d0af-465e-9b5d-abdac83a6f2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50280, 2560)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf62f72-d86b-4a2c-ae3d-2d2f3c5948aa",
   "metadata": {},
   "source": [
    "We use the `preprocess_batch` function to preprocess the \"text\" field of the batch, applying tokenization, truncation, and other relevant operations based on the specified maximum length. It takes a batch of data, a tokenizer, and a maximum length as input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33b795e5-6c39-4382-ac1d-95a10b560dfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import copy\n",
    "\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "# Function to generate token embeddings\n",
    "def _preprocess_batch(batch: Dict[str, List]):  \n",
    "    model_inputs = tokenizer(batch[\"text\"], max_length=MAX_LENGTH, truncation=True, padding='max_length')\n",
    "    \n",
    "    # Labels will be the same as the input as this is a language modeling task\n",
    "    model_inputs[\"labels\"] = copy.deepcopy(model_inputs['input_ids'])\n",
    "    return model_inputs\n",
    "\n",
    "_preprocessing_function = partial(_preprocess_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38f52e4-cdaa-4c26-b0d5-5578ca5a1b04",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next, we apply the preprocessing function to each batch in the dataset, modifying the \"text\" field accordingly. The map operation is performed in a batched manner and the \"instruction\", \"response\", and \"text\" columns are removed from the dataset. Finally, the `processed_dataset` is created by filtering the `sagemaker_faqs_dataset` based on the length of the \"input_ids\" field, ensuring it is less than the specified `MAX_LENGTH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a160182-6f42-4e4f-b06f-d758c77d4ea7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df30f0930c1f48cc854b7366b54dfd56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f02dced6540141b392ac2de36c6f3077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_sagemaker_faqs_dataset = sagemaker_faqs_dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"instruction\", \"response\", \"prompt\", \"answer\"],\n",
    ")\n",
    "\n",
    "processed_dataset = encoded_sagemaker_faqs_dataset.filter(lambda rec: len(rec[\"input_ids\"]) <= MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f5b18f-f6dc-4cdb-a8eb-badc2e5f846b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's split dataset into `train` and `test` for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9440fe26-1e2a-4a0c-bd58-6bf712b226a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 140\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 14\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset = processed_dataset.train_test_split(test_size=14, seed=0)\n",
    "split_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b422e14d-a47e-4b79-9433-b63c6f0e8903",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### <a name=\"4\">Define the trainer and finetuned the LLM</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "To finetune a model efficiently, we're going to use [LoRA: Low-Rank Adaptation](https://arxiv.org/abs/2106.09685). LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. \n",
    "\n",
    "\n",
    "#### 1). Define the `LoraConfig` and load LoRA model\n",
    "\n",
    "We'll us built LoRA class `LoraConfig` from [huggingface ðŸ¤— PEFT: State-of-the-art Parameter-Efficient Fine-Tuning](https://github.com/huggingface/peft). Within `LoraConfig`, let's specify the following parameters:\n",
    "\n",
    "- `r`, the dimension of the low-rank matrices\n",
    "- `lora_alpha`, the scaling factor for the low-rank matrices\n",
    "- `lora_dropout`, the dropout probability of the LoRA layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2f08ad8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType, PeftConfig, PeftModel\n",
    "\n",
    "MICRO_BATCH_SIZE = 8  \n",
    "BATCH_SIZE = 64\n",
    "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
    "LORA_R = 8 # 512\n",
    "LORA_ALPHA = 32 # 1024\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "# Define LoRA Config\n",
    "lora_config = LoraConfig(\n",
    "                 r=LORA_R, # LoRA attention dimension\n",
    "                 lora_alpha=LORA_ALPHA, #  Alpha parameter for LoRA scaling\n",
    "                 lora_dropout=LORA_DROPOUT, # Dropout probability \n",
    "                 bias=\"none\",\n",
    "                 task_type=\"CAUSAL_LM\",\n",
    "                target_modules=[\"query_key_value\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484b0130-b221-425b-a30d-ac4e14fbf355",
   "metadata": {},
   "source": [
    "Let's use the `get_peft_model` function to initialize the model with the LoRA framework, configuring it based on the provided `lora_config` settings. This allows the model to incorporate the benefits and capabilities of the LoRA optimization approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c57df3d9-81f0-47d1-80a2-0b7f6dbc6d19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2621440 || all params: 2777707520 || trainable%: 0.09437422698844837\n"
     ]
    }
   ],
   "source": [
    "# Prepare int-8 model for training\n",
    "model = prepare_model_for_int8_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb474fb-3bb3-4ad5-84b5-312543e0d8be",
   "metadata": {},
   "source": [
    "As we can see above, LoRA only trainable parameters is only ~3% of the full weights. Much efficient!\n",
    "\n",
    "#### 2). Define the data collator\n",
    "\n",
    "A DataCollator is a huggingfaceðŸ¤— transformers function that takes a list of samples from a Dataset and collate them into a batch, as a dictionary of PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dabc4678-a6a2-4549-91b0-3c7ebddc50ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq, DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, pad_to_multiple_of=8, mlm=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43ac881-bf08-4f91-a33f-a76dd36f5e84",
   "metadata": {},
   "source": [
    "#### 3). Define the trainer\n",
    "\n",
    "To finetune the LLM, we need to define a trainer. Let's define the training arguments first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46d94157-d0cc-4bfa-ba7b-148de58e1ddb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 3\n",
    "LEARNING_RATE = 1e-4  \n",
    "MODEL_SAVE_FOLDER_NAME = \"dolly-3b-lora\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "                    output_dir=MODEL_SAVE_FOLDER_NAME,\n",
    "                    overwrite_output_dir=True,\n",
    "                    fp16=True,\n",
    "                    per_device_train_batch_size=1,\n",
    "                    per_device_eval_batch_size=1,\n",
    "                    learning_rate=LEARNING_RATE,\n",
    "                    # optim=\"adafactor\",\n",
    "                    num_train_epochs=EPOCHS,\n",
    "                    logging_strategy=\"epoch\",\n",
    "                    evaluation_strategy=\"epoch\",\n",
    "                    save_strategy=\"epoch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba9b264-016d-45a7-9106-9e847e1b8a71",
   "metadata": {},
   "source": [
    "Now is where the magic happen! Let's initialize the trainer with our defined model, tokenizer, training arguments, data collator and the train/eval datasets. \n",
    "\n",
    "The training may take ~15 minutes. Once the training is done, we save the finetuned model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eee4d91d-01a1-44a9-abef-8769f22f40ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='420' max='420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [420/420 06:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.906100</td>\n",
       "      <td>1.668145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.550500</td>\n",
       "      <td>1.621012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.425800</td>\n",
       "      <td>1.609697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 31s, sys: 465 ms, total: 6min 32s\n",
      "Wall time: 6min 31s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=420, training_loss=1.6274852934337798, metrics={'train_runtime': 391.6213, 'train_samples_per_second': 1.072, 'train_steps_per_second': 1.072, 'total_flos': 1708916893286400.0, 'train_loss': 1.6274852934337798, 'epoch': 3.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        train_dataset=split_dataset['train'],\n",
    "        eval_dataset=split_dataset[\"test\"],\n",
    "        data_collator=data_collator,\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ce987a-f306-4f98-908c-29939f311d4c",
   "metadata": {},
   "source": [
    "#### <a>Save the finetuned model</a>\n",
    "\n",
    "\n",
    "After the training finished, we can save the model to a directory using the [`transformers.PreTrainedModel.save_pretrained`] function. \n",
    "This function only saves the incremental ðŸ¤— PEFT weights (adapter_model.bin) that were trained, meaning \n",
    "it is super efficient to store, transfer, and load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5715f6ee-ac1d-4055-9e3a-395b5a52dd2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trainer.model.save_pretrained(MODEL_SAVE_FOLDER_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7aa3c5-acff-4704-afe1-c85707509b8e",
   "metadata": {},
   "source": [
    "If you want to save the full model you just finetuned, you can simply use the [`transformers.trainer.save_model`] function. Meanwhile, we save the training arguments together with the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7dd9581b-4944-4c83-af5c-70a216fee4d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.save_model(MODEL_SAVE_FOLDER_NAME)\n",
    "trainer.model.config.save_pretrained(MODEL_SAVE_FOLDER_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b5a5a9-884b-4852-acc2-89f214df62a5",
   "metadata": {},
   "source": [
    "As you can see from the losses above, the training loss dropped quickly while the validation loss doesn't improve at all. This indicates that our training data and validation data are quite different. As a result, our model is overfitting to the training data and is not able to generalize well to unseen examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b03e87",
   "metadata": {},
   "source": [
    "\n",
    "### <a name=\"5\">Inference using the finetuned model</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Let's use a text generation pipeline for instruction text generation using the Huggingface transformers library. \n",
    "\n",
    "The `postprocess` function formats the generated sequence to extract the response text. It uses 'Response:' to locate and extract the response portion from the generated tokens. If the key is not found it throws a ValueError."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5444140b-6af5-49c1-854a-e3fe7e2760f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to format the response and filter out the instruction from the response.\n",
    "def postprocess(response):\n",
    "    messages = response.split(\"Response:\")\n",
    "    if not messages:\n",
    "        raise ValueError(\"Invalid template for prompt. The template should include the term 'Response:'\")\n",
    "    return \"\".join(messages[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030a30cf-10b2-42cb-8999-5e53726f1f2e",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
    "    <h2><i>Try it Yourself!</i></h2>\n",
    "    <br>\n",
    "    <p style=\"text-align:center;margin:auto;\"><img src=\"./images/activity.png\" alt=\"Activity\" width=\"100\" /> </p>\n",
    "    <p style=\" text-align: center; margin: auto;\">Compare the responses of the fine-tuned model against the vanilla pre-trained LLM.</p>\n",
    "    <p style=\" text-align: center; margin: auto;\"><b>Note: Results may not be factually accurate and may be based on false assumptions.</b></p>\n",
    "    <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db1daaf8-aad8-49e7-8672-333322cd2827",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prompt for prediction\n",
    "inference_prompt = \"What solutions come pre-built with Amazon SageMaker JumpStart?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "678b08d9-6327-42c9-acbb-790aba1ac613",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Inference pipeline with the fine-tuned model\n",
    "inf_pipeline =  pipeline('text-generation', model=trainer.model, tokenizer=tokenizer, max_length=256, trust_remote_code=True, do_sample=True)\n",
    "\n",
    "# Format the prompt using the `prompt_template` and generate response \n",
    "response = inf_pipeline(prompt_template.format(instruction=inference_prompt))[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2978e5e-1659-4ba7-9435-e74e2545f91e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We provide pre-built solutions that we designed specifically with machine learning in mind within a unified framework for both deep learning and traditional ML practitioners. The solutions have been professionally evaluated to ensure the best performance using data from ML experiments. Once you enable the solutions from the SageMaker dashboard, we automatically create training jobs across various major ML models and frameworks, including AWS Neo and AWS SageMaker Studio. To get started, visit the SageMaker JumpStart website and log into the dashboard to get started. You can also find more information at the Amazon Recommender Dashboard page. Note that all solutions are provided in a unified framework, where you can also configure your models, frameworks, and tasks within one project. This allows you to quickly and easily configure your models and frameworks from the SageMaker Dashboard with a few clicks. Each solution has been carefully optimized to provide maximum performance within each framework. SageMaker JumpStart fully automates the creation of training jobs, monitoring and scaling, and ML model development and deployment. Once you enable the solution,'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_response = postprocess(response)\n",
    "formatted_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d67bf64-eb33-4504-bbf8-d9bfd922a9e4",
   "metadata": {},
   "source": [
    "### <a name=\"6\">Quizzes</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Well done on completing the lab! Now, it's time for a brief knowledge assessment.\n",
    "\n",
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
    "    <h2><i>Try it Yourself!</i></h2>\n",
    "    <br>\n",
    "    <p style=\"text-align:center;margin:auto;\"><img src=\"./images/challenge.png\" alt=\"Challenge\" width=\"100\" /> </p>\n",
    "    <p style=\" text-align: center; margin: auto;\">Answer the following questions to test your understanding of fine-tuning LLMs.</p>\n",
    "    <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2bd1226b-3b80-4054-8d94-5f2fa4b77b6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>var Quiz=function(){\"use strict\";var M=document.createElement(\"style\");M.textContent=`.quiz-wrapper.svelte-fk6ar3{padding:1rem}.footer.svelte-fk6ar3{display:flex;align-items:center}h2.svelte-fk6ar3{font-size:1.5rem;margin-bottom:2rem;color:#232f3e}p.svelte-fk6ar3{font-size:16px}.options.svelte-fk6ar3{display:grid;grid-template-columns:repeat(2,50%);grid-template-rows:repeat(2,1fr);width:100%;margin:auto;justify-content:center}.mlu-quizquestion-option-button.svelte-fk6ar3{padding:1rem;margin:.5rem}.submit-button.svelte-fk6ar3{padding:1rem;margin:.5rem;width:90px;color:#fff;background-color:coral}.active.svelte-fk6ar3{background-color:#232f3e;color:#fff}.correct-answer.svelte-fk6ar3{background-color:green}.incorrect-answer.svelte-fk6ar3{background-color:red;text-decoration:line-through}.available.svelte-fk6ar3{pointer-events:none;opacity:.6}\n",
       "`,document.head.appendChild(M);function I(){}function P(e){return e()}function W(){return Object.create(null)}function O(e){e.forEach(P)}function X(e){return typeof e==\"function\"}function Z(e,t){return e!=e?t==t:e!==t||e&&typeof e==\"object\"||typeof e==\"function\"}function x(e){return Object.keys(e).length===0}function m(e,t){e.appendChild(t)}function j(e,t,n){e.insertBefore(t,n||null)}function z(e){e.parentNode&&e.parentNode.removeChild(e)}function ee(e,t){for(let n=0;n<e.length;n+=1)e[n]&&e[n].d(t)}function k(e){return document.createElement(e)}function A(e){return document.createTextNode(e)}function S(){return A(\" \")}function te(){return A(\"\")}function Y(e,t,n,r){return e.addEventListener(t,n,r),()=>e.removeEventListener(t,n,r)}function v(e,t,n){n==null?e.removeAttribute(t):e.getAttribute(t)!==n&&e.setAttribute(t,n)}function ne(e){return Array.from(e.childNodes)}function T(e,t){t=\"\"+t,e.wholeText!==t&&(e.data=t)}function p(e,t,n){e.classList[n?\"add\":\"remove\"](t)}let L;function N(e){L=e}const $=[],D=[],Q=[],F=[],re=Promise.resolve();let B=!1;function oe(){B||(B=!0,re.then(H))}function R(e){Q.push(e)}const G=new Set;let E=0;function H(){if(E!==0)return;const e=L;do{try{for(;E<$.length;){const t=$[E];E++,N(t),le(t.$$)}}catch(t){throw $.length=0,E=0,t}for(N(null),$.length=0,E=0;D.length;)D.pop()();for(let t=0;t<Q.length;t+=1){const n=Q[t];G.has(n)||(G.add(n),n())}Q.length=0}while($.length);for(;F.length;)F.pop()();B=!1,G.clear(),N(e)}function le(e){if(e.fragment!==null){e.update(),O(e.before_update);const t=e.dirty;e.dirty=[-1],e.fragment&&e.fragment.p(e.ctx,t),e.after_update.forEach(R)}}const ie=new Set;function se(e,t){e&&e.i&&(ie.delete(e),e.i(t))}function ce(e,t,n,r){const{fragment:l,after_update:i}=e.$$;l&&l.m(t,n),r||R(()=>{const s=e.$$.on_mount.map(P).filter(X);e.$$.on_destroy?e.$$.on_destroy.push(...s):O(s),e.$$.on_mount=[]}),i.forEach(R)}function fe(e,t){const n=e.$$;n.fragment!==null&&(O(n.on_destroy),n.fragment&&n.fragment.d(t),n.on_destroy=n.fragment=null,n.ctx=[])}function ue(e,t){e.$$.dirty[0]===-1&&($.push(e),oe(),e.$$.dirty.fill(0)),e.$$.dirty[t/31|0]|=1<<t%31}function ae(e,t,n,r,l,i,s,g=[-1]){const c=L;N(e);const o=e.$$={fragment:null,ctx:[],props:i,update:I,not_equal:l,bound:W(),on_mount:[],on_destroy:[],on_disconnect:[],before_update:[],after_update:[],context:new Map(t.context||(c?c.$$.context:[])),callbacks:W(),dirty:g,skip_bound:!1,root:t.target||c.$$.root};s&&s(o.root);let b=!1;if(o.ctx=n?n(e,t.props||{},(a,q,...y)=>{const h=y.length?y[0]:q;return o.ctx&&l(o.ctx[a],o.ctx[a]=h)&&(!o.skip_bound&&o.bound[a]&&o.bound[a](h),b&&ue(e,a)),q}):[],o.update(),b=!0,O(o.before_update),o.fragment=r?r(o.ctx):!1,t.target){if(t.hydrate){const a=ne(t.target);o.fragment&&o.fragment.l(a),a.forEach(z)}else o.fragment&&o.fragment.c();t.intro&&se(e.$$.fragment),ce(e,t.target,t.anchor,t.customElement),H()}N(c)}class de{$destroy(){fe(this,1),this.$destroy=I}$on(t,n){if(!X(n))return I;const r=this.$$.callbacks[t]||(this.$$.callbacks[t]=[]);return r.push(n),()=>{const l=r.indexOf(n);l!==-1&&r.splice(l,1)}}$set(t){this.$$set&&!x(t)&&(this.$$.skip_bound=!0,this.$$set(t),this.$$.skip_bound=!1)}}const be=\"\";function J(e,t,n){const r=e.slice();return r[11]=t[n],r[13]=n,r}function K(e){let t,n=e[11]+\"\",r,l,i,s;function g(){return e[9](e[13])}return{c(){t=k(\"button\"),r=A(n),l=S(),v(t,\"class\",\"mlu-quizquestion-option-button button svelte-fk6ar3\"),p(t,\"active\",e[5]===e[13]),p(t,\"correct-answer\",e[1]&&e[5]===e[13]&&e[2]==e[0].correctIndex),p(t,\"incorrect-answer\",e[1]&&e[5]===e[13]&&e[2]!=e[0].correctIndex),p(t,\"available\",e[4]&&e[1])},m(c,o){j(c,t,o),m(t,r),m(t,l),i||(s=Y(t,\"click\",g),i=!0)},p(c,o){e=c,o&1&&n!==(n=e[11]+\"\")&&T(r,n),o&32&&p(t,\"active\",e[5]===e[13]),o&39&&p(t,\"correct-answer\",e[1]&&e[5]===e[13]&&e[2]==e[0].correctIndex),o&39&&p(t,\"incorrect-answer\",e[1]&&e[5]===e[13]&&e[2]!=e[0].correctIndex),o&18&&p(t,\"available\",e[4]&&e[1])},d(c){c&&z(t),i=!1,s()}}}function U(e){let t;function n(i,s){return i[3]==!0?he:_e}let r=n(e),l=r(e);return{c(){l.c(),t=te()},m(i,s){l.m(i,s),j(i,t,s)},p(i,s){r!==(r=n(i))&&(l.d(1),l=r(i),l&&(l.c(),l.m(t.parentNode,t)))},d(i){l.d(i),i&&z(t)}}}function _e(e){let t;return{c(){t=k(\"p\"),t.textContent=\"This is not the correct answer. Try again!\",v(t,\"class\",\"svelte-fk6ar3\")},m(n,r){j(n,t,r)},d(n){n&&z(t)}}}function he(e){let t;return{c(){t=k(\"p\"),t.textContent=\"Good! You got the correct answer.\",v(t,\"class\",\"svelte-fk6ar3\")},m(n,r){j(n,t,r)},d(n){n&&z(t)}}}function me(e){let t,n,r=e[0].question+\"\",l,i,s,g,c,o,b=e[1]?\"Retry\":\"Submit\",a,q,y,h,C=e[0].options,d=[];for(let f=0;f<C.length;f+=1)d[f]=K(J(e,C,f));let _=e[1]&&U(e);return{c(){t=k(\"div\"),n=k(\"h2\"),l=A(r),i=S(),s=k(\"div\");for(let f=0;f<d.length;f+=1)d[f].c();g=S(),c=k(\"div\"),o=k(\"button\"),a=A(b),q=S(),_&&_.c(),v(n,\"class\",\"svelte-fk6ar3\"),v(s,\"class\",\"options svelte-fk6ar3\"),v(o,\"class\",\"submit-button svelte-fk6ar3\"),p(o,\"available\",!e[4]),v(c,\"class\",\"footer svelte-fk6ar3\"),v(t,\"class\",\"quiz-wrapper svelte-fk6ar3\")},m(f,w){j(f,t,w),m(t,n),m(n,l),m(t,i),m(t,s);for(let u=0;u<d.length;u+=1)d[u].m(s,null);m(t,g),m(t,c),m(c,o),m(o,a),m(c,q),_&&_.m(c,null),y||(h=Y(o,\"click\",e[10]),y=!0)},p(f,[w]){if(w&1&&r!==(r=f[0].question+\"\")&&T(l,r),w&311){C=f[0].options;let u;for(u=0;u<C.length;u+=1){const V=J(f,C,u);d[u]?d[u].p(V,w):(d[u]=K(V),d[u].c(),d[u].m(s,null))}for(;u<d.length;u+=1)d[u].d(1);d.length=C.length}w&2&&b!==(b=f[1]?\"Retry\":\"Submit\")&&T(a,b),w&16&&p(o,\"available\",!f[4]),f[1]?_?_.p(f,w):(_=U(f),_.c(),_.m(c,null)):_&&(_.d(1),_=null)},i:I,o:I,d(f){f&&z(t),ee(d,f),_&&_.d(),y=!1,h()}}}function pe(e,t,n){let{question:r={question:\"Who didn't attend this meeting?\",options:[\"Xin\",\"Anand\",\"Brent\"],correctIndex:2}}=t,l=!1,i=-1,s=\"no\",g=!1,c;function o(){n(4,g=!1),n(1,l=!1),n(2,i=-1),n(5,c=-1),n(3,s=\"no\")}function b(){n(1,l=!0),n(3,s=i==r.correctIndex)}function a(h){n(4,g=!0),n(2,i=h),n(5,c=h)}const q=h=>a(h),y=()=>l?o():b();return e.$$set=h=>{\"question\"in h&&n(0,r=h.question)},[r,l,i,s,g,c,o,b,a,q,y]}class ge extends de{constructor(t){super(),ae(this,t,pe,me,Z,{question:0})}}return ge}();\n",
       "</script>\n",
       "        \n",
       "        <div id=\"Quiz-78fa7b4\"></div>\n",
       "        <script>\n",
       "        (() => {\n",
       "            var data = {\n",
       "\"question\": {\n",
       "\"question\": \"Which of the following techniques was used in the notebook for memory efficient fine-tuning of an LLM?\",\n",
       "\"options\": [\n",
       "\"Adding special tokens to tokenizer\",\n",
       "\"Initialize the base model using AutoModelForCausalLM\",\n",
       "\"Low-Rank Adaptation (LoRA)\",\n",
       "\"Setting device to 'auto'\"\n",
       "],\n",
       "\"correctIndex\": 2\n",
       "}\n",
       "};\n",
       "            window.Quiz_data = data;\n",
       "            var Quiz_inst = new Quiz({\n",
       "                \"target\": document.getElementById(\"Quiz-78fa7b4\"),\n",
       "                \"props\": data\n",
       "            });\n",
       "        })();\n",
       "        </script>\n",
       "        \n",
       "        "
      ],
      "text/plain": [
       "<mlu_utils.quiz_questions.Quiz at 0x7fb68818fe50>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlu_utils.quiz_questions import *\n",
    "lab2_question1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "478315bd-590b-4f28-90a0-c6b79505c114",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>var Quiz=function(){\"use strict\";var M=document.createElement(\"style\");M.textContent=`.quiz-wrapper.svelte-fk6ar3{padding:1rem}.footer.svelte-fk6ar3{display:flex;align-items:center}h2.svelte-fk6ar3{font-size:1.5rem;margin-bottom:2rem;color:#232f3e}p.svelte-fk6ar3{font-size:16px}.options.svelte-fk6ar3{display:grid;grid-template-columns:repeat(2,50%);grid-template-rows:repeat(2,1fr);width:100%;margin:auto;justify-content:center}.mlu-quizquestion-option-button.svelte-fk6ar3{padding:1rem;margin:.5rem}.submit-button.svelte-fk6ar3{padding:1rem;margin:.5rem;width:90px;color:#fff;background-color:coral}.active.svelte-fk6ar3{background-color:#232f3e;color:#fff}.correct-answer.svelte-fk6ar3{background-color:green}.incorrect-answer.svelte-fk6ar3{background-color:red;text-decoration:line-through}.available.svelte-fk6ar3{pointer-events:none;opacity:.6}\n",
       "`,document.head.appendChild(M);function I(){}function P(e){return e()}function W(){return Object.create(null)}function O(e){e.forEach(P)}function X(e){return typeof e==\"function\"}function Z(e,t){return e!=e?t==t:e!==t||e&&typeof e==\"object\"||typeof e==\"function\"}function x(e){return Object.keys(e).length===0}function m(e,t){e.appendChild(t)}function j(e,t,n){e.insertBefore(t,n||null)}function z(e){e.parentNode&&e.parentNode.removeChild(e)}function ee(e,t){for(let n=0;n<e.length;n+=1)e[n]&&e[n].d(t)}function k(e){return document.createElement(e)}function A(e){return document.createTextNode(e)}function S(){return A(\" \")}function te(){return A(\"\")}function Y(e,t,n,r){return e.addEventListener(t,n,r),()=>e.removeEventListener(t,n,r)}function v(e,t,n){n==null?e.removeAttribute(t):e.getAttribute(t)!==n&&e.setAttribute(t,n)}function ne(e){return Array.from(e.childNodes)}function T(e,t){t=\"\"+t,e.wholeText!==t&&(e.data=t)}function p(e,t,n){e.classList[n?\"add\":\"remove\"](t)}let L;function N(e){L=e}const $=[],D=[],Q=[],F=[],re=Promise.resolve();let B=!1;function oe(){B||(B=!0,re.then(H))}function R(e){Q.push(e)}const G=new Set;let E=0;function H(){if(E!==0)return;const e=L;do{try{for(;E<$.length;){const t=$[E];E++,N(t),le(t.$$)}}catch(t){throw $.length=0,E=0,t}for(N(null),$.length=0,E=0;D.length;)D.pop()();for(let t=0;t<Q.length;t+=1){const n=Q[t];G.has(n)||(G.add(n),n())}Q.length=0}while($.length);for(;F.length;)F.pop()();B=!1,G.clear(),N(e)}function le(e){if(e.fragment!==null){e.update(),O(e.before_update);const t=e.dirty;e.dirty=[-1],e.fragment&&e.fragment.p(e.ctx,t),e.after_update.forEach(R)}}const ie=new Set;function se(e,t){e&&e.i&&(ie.delete(e),e.i(t))}function ce(e,t,n,r){const{fragment:l,after_update:i}=e.$$;l&&l.m(t,n),r||R(()=>{const s=e.$$.on_mount.map(P).filter(X);e.$$.on_destroy?e.$$.on_destroy.push(...s):O(s),e.$$.on_mount=[]}),i.forEach(R)}function fe(e,t){const n=e.$$;n.fragment!==null&&(O(n.on_destroy),n.fragment&&n.fragment.d(t),n.on_destroy=n.fragment=null,n.ctx=[])}function ue(e,t){e.$$.dirty[0]===-1&&($.push(e),oe(),e.$$.dirty.fill(0)),e.$$.dirty[t/31|0]|=1<<t%31}function ae(e,t,n,r,l,i,s,g=[-1]){const c=L;N(e);const o=e.$$={fragment:null,ctx:[],props:i,update:I,not_equal:l,bound:W(),on_mount:[],on_destroy:[],on_disconnect:[],before_update:[],after_update:[],context:new Map(t.context||(c?c.$$.context:[])),callbacks:W(),dirty:g,skip_bound:!1,root:t.target||c.$$.root};s&&s(o.root);let b=!1;if(o.ctx=n?n(e,t.props||{},(a,q,...y)=>{const h=y.length?y[0]:q;return o.ctx&&l(o.ctx[a],o.ctx[a]=h)&&(!o.skip_bound&&o.bound[a]&&o.bound[a](h),b&&ue(e,a)),q}):[],o.update(),b=!0,O(o.before_update),o.fragment=r?r(o.ctx):!1,t.target){if(t.hydrate){const a=ne(t.target);o.fragment&&o.fragment.l(a),a.forEach(z)}else o.fragment&&o.fragment.c();t.intro&&se(e.$$.fragment),ce(e,t.target,t.anchor,t.customElement),H()}N(c)}class de{$destroy(){fe(this,1),this.$destroy=I}$on(t,n){if(!X(n))return I;const r=this.$$.callbacks[t]||(this.$$.callbacks[t]=[]);return r.push(n),()=>{const l=r.indexOf(n);l!==-1&&r.splice(l,1)}}$set(t){this.$$set&&!x(t)&&(this.$$.skip_bound=!0,this.$$set(t),this.$$.skip_bound=!1)}}const be=\"\";function J(e,t,n){const r=e.slice();return r[11]=t[n],r[13]=n,r}function K(e){let t,n=e[11]+\"\",r,l,i,s;function g(){return e[9](e[13])}return{c(){t=k(\"button\"),r=A(n),l=S(),v(t,\"class\",\"mlu-quizquestion-option-button button svelte-fk6ar3\"),p(t,\"active\",e[5]===e[13]),p(t,\"correct-answer\",e[1]&&e[5]===e[13]&&e[2]==e[0].correctIndex),p(t,\"incorrect-answer\",e[1]&&e[5]===e[13]&&e[2]!=e[0].correctIndex),p(t,\"available\",e[4]&&e[1])},m(c,o){j(c,t,o),m(t,r),m(t,l),i||(s=Y(t,\"click\",g),i=!0)},p(c,o){e=c,o&1&&n!==(n=e[11]+\"\")&&T(r,n),o&32&&p(t,\"active\",e[5]===e[13]),o&39&&p(t,\"correct-answer\",e[1]&&e[5]===e[13]&&e[2]==e[0].correctIndex),o&39&&p(t,\"incorrect-answer\",e[1]&&e[5]===e[13]&&e[2]!=e[0].correctIndex),o&18&&p(t,\"available\",e[4]&&e[1])},d(c){c&&z(t),i=!1,s()}}}function U(e){let t;function n(i,s){return i[3]==!0?he:_e}let r=n(e),l=r(e);return{c(){l.c(),t=te()},m(i,s){l.m(i,s),j(i,t,s)},p(i,s){r!==(r=n(i))&&(l.d(1),l=r(i),l&&(l.c(),l.m(t.parentNode,t)))},d(i){l.d(i),i&&z(t)}}}function _e(e){let t;return{c(){t=k(\"p\"),t.textContent=\"This is not the correct answer. Try again!\",v(t,\"class\",\"svelte-fk6ar3\")},m(n,r){j(n,t,r)},d(n){n&&z(t)}}}function he(e){let t;return{c(){t=k(\"p\"),t.textContent=\"Good! You got the correct answer.\",v(t,\"class\",\"svelte-fk6ar3\")},m(n,r){j(n,t,r)},d(n){n&&z(t)}}}function me(e){let t,n,r=e[0].question+\"\",l,i,s,g,c,o,b=e[1]?\"Retry\":\"Submit\",a,q,y,h,C=e[0].options,d=[];for(let f=0;f<C.length;f+=1)d[f]=K(J(e,C,f));let _=e[1]&&U(e);return{c(){t=k(\"div\"),n=k(\"h2\"),l=A(r),i=S(),s=k(\"div\");for(let f=0;f<d.length;f+=1)d[f].c();g=S(),c=k(\"div\"),o=k(\"button\"),a=A(b),q=S(),_&&_.c(),v(n,\"class\",\"svelte-fk6ar3\"),v(s,\"class\",\"options svelte-fk6ar3\"),v(o,\"class\",\"submit-button svelte-fk6ar3\"),p(o,\"available\",!e[4]),v(c,\"class\",\"footer svelte-fk6ar3\"),v(t,\"class\",\"quiz-wrapper svelte-fk6ar3\")},m(f,w){j(f,t,w),m(t,n),m(n,l),m(t,i),m(t,s);for(let u=0;u<d.length;u+=1)d[u].m(s,null);m(t,g),m(t,c),m(c,o),m(o,a),m(c,q),_&&_.m(c,null),y||(h=Y(o,\"click\",e[10]),y=!0)},p(f,[w]){if(w&1&&r!==(r=f[0].question+\"\")&&T(l,r),w&311){C=f[0].options;let u;for(u=0;u<C.length;u+=1){const V=J(f,C,u);d[u]?d[u].p(V,w):(d[u]=K(V),d[u].c(),d[u].m(s,null))}for(;u<d.length;u+=1)d[u].d(1);d.length=C.length}w&2&&b!==(b=f[1]?\"Retry\":\"Submit\")&&T(a,b),w&16&&p(o,\"available\",!f[4]),f[1]?_?_.p(f,w):(_=U(f),_.c(),_.m(c,null)):_&&(_.d(1),_=null)},i:I,o:I,d(f){f&&z(t),ee(d,f),_&&_.d(),y=!1,h()}}}function pe(e,t,n){let{question:r={question:\"Who didn't attend this meeting?\",options:[\"Xin\",\"Anand\",\"Brent\"],correctIndex:2}}=t,l=!1,i=-1,s=\"no\",g=!1,c;function o(){n(4,g=!1),n(1,l=!1),n(2,i=-1),n(5,c=-1),n(3,s=\"no\")}function b(){n(1,l=!0),n(3,s=i==r.correctIndex)}function a(h){n(4,g=!0),n(2,i=h),n(5,c=h)}const q=h=>a(h),y=()=>l?o():b();return e.$$set=h=>{\"question\"in h&&n(0,r=h.question)},[r,l,i,s,g,c,o,b,a,q,y]}class ge extends de{constructor(t){super(),ae(this,t,pe,me,Z,{question:0})}}return ge}();\n",
       "</script>\n",
       "        \n",
       "        <div id=\"Quiz-356b713e\"></div>\n",
       "        <script>\n",
       "        (() => {\n",
       "            var data = {\n",
       "\"question\": {\n",
       "\"question\": \"What is the purpose of DataCollatorForLanguageModeling?\",\n",
       "\"options\": [\n",
       "\"It is a function that takes a list of samples from a Dataset and collate them into a batch, as a dictionary of PyTorch tensors\",\n",
       "\"It is a function that initializes the configurations for LoRA\",\n",
       "\"It is the main training function which optimizes the weights of the model\",\n",
       "\"It is a preprocessing function which transforms the prompt in the appropriate format for instruction fine-tuning\"\n",
       "],\n",
       "\"correctIndex\": 0\n",
       "}\n",
       "};\n",
       "            window.Quiz_data = data;\n",
       "            var Quiz_inst = new Quiz({\n",
       "                \"target\": document.getElementById(\"Quiz-356b713e\"),\n",
       "                \"props\": data\n",
       "            });\n",
       "        })();\n",
       "        </script>\n",
       "        \n",
       "        "
      ],
      "text/plain": [
       "<mlu_utils.quiz_questions.Quiz at 0x7fb68818f9a0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab2_question2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673c3eb6-c769-4b28-aed1-500fe54a892c",
   "metadata": {},
   "source": [
    "Let's restart the kernel to release CPU and GPU memory for the next lab\n",
    "\n",
    "**You might see a pop up indicating the kernel has been restarted.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d83b8a66-3480-425c-893e-c22617eed74b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import IPython\n",
    "# IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b85c55d-b6b0-4731-a324-6ff89cc42835",
   "metadata": {},
   "source": [
    "<p style=\"padding: 10px; border: 1px solid black;\">\n",
    "<img src=\"images/MLU-NEW-logo.png\" alt=\"drawing\" width=\"400\"/> <br/>\n",
    "\n",
    "# Thank you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b78031c-33b6-4985-a8d5-5137d02a629a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
